{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ARENA Chapter 1.1: Complete Transformer from Scratch\n\n**Educational Implementation for Mechanistic Interpretability**\n\nThis notebook provides a comprehensive demonstration of our educational transformer implementation, following the ARENA Chapter 1.1 curriculum. We'll explore:\n\n- Mathematical foundations\n- Token and position embeddings  \n- Multi-head attention mechanisms\n- MLP blocks with GELU activation\n- Complete transformer architecture\n- Mechanistic interpretability tools\n- Educational analysis and visualization\n\n**Learning Objectives:**\n- Understand transformer architecture from first principles\n- Gain intuition for attention mechanisms and residual streams\n- Learn mechanistic interpretability techniques\n- Build practical skills with TransformerLens-compatible implementations"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# Add src to path\nsys.path.append('..')\n\n# Import our educational transformer components\nfrom src.models.config import TransformerConfig\nfrom src.models.transformer import EducationalTransformer\nfrom src.foundations.attention_math import AttentionMathematics\nfrom src.foundations.position_encoding import SinusoidalPositionEncoding\nfrom src.components.mlp import GELU\nfrom src.interpretability.hooks import HookManager, ActivationPatcher, AblationAnalyzer\n\n# Set style for better plots\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"Educational Transformer Implementation\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Part 1: Mathematical Foundations\n",
    "\n",
    "Let's start by understanding the core mathematical operations that make transformers work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¢ Mathematical Foundations Demo\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Demo 1: Scaled Dot-Product Attention Math\n",
    "print(\"\\n1. Scaled Dot-Product Attention\")\n",
    "\n",
    "batch_size, n_heads, seq_len, d_head = 1, 1, 4, 8\n",
    "\n",
    "# Create simple, interpretable Q, K, V\n",
    "query = torch.tensor([[\n",
    "    [[1., 0., 0., 0., 0., 0., 0., 0.]],  # Token 0: looking for \"first\" feature\n",
    "    [[0., 1., 0., 0., 0., 0., 0., 0.]],  # Token 1: looking for \"second\" feature\n",
    "    [[0., 0., 1., 0., 0., 0., 0., 0.]],  # Token 2: looking for \"third\" feature\n",
    "    [[0., 0., 0., 1., 0., 0., 0., 0.]]   # Token 3: looking for \"fourth\" feature\n",
    "]])\n",
    "\n",
    "key = torch.tensor([[\n",
    "    [[1., 0., 0., 0., 0., 0., 0., 0.]],    # Token 0: has \"first\" feature\n",
    "    [[0., 1., 0., 0., 0., 0., 0., 0.]],    # Token 1: has \"second\" feature  \n",
    "    [[0., 0., 1., 0., 0., 0., 0., 0.]],    # Token 2: has \"third\" feature\n",
    "    [[0.5, 0.5, 0., 1., 0., 0., 0., 0.]]  # Token 3: has mixed features\n",
    "]])\n",
    "\n",
    "value = torch.tensor([[\n",
    "    [[1., 2., 3., 4., 0., 0., 0., 0.]],  # Token 0: valuable info A\n",
    "    [[5., 6., 7., 8., 0., 0., 0., 0.]],  # Token 1: valuable info B\n",
    "    [[9., 10., 11., 12., 0., 0., 0., 0.]],  # Token 2: valuable info C\n",
    "    [[13., 14., 15., 16., 0., 0., 0., 0.]]  # Token 3: valuable info D\n",
    "]])\n",
    "\n",
    "# Compute attention\n",
    "output, attention_weights, debug_info = AttentionMathematics.single_head_attention(\n",
    "    query, key, value\n",
    ")\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Attention weights:\")\n",
    "print(attention_weights.squeeze().round(decimals=3))\n",
    "print(f\"\\nOutput (first 4 values):\")\n",
    "print(output.squeeze()[:, :4].round(decimals=2))\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- Token 0 attends to itself (1.0) ‚Üí gets its own value\")\n",
    "print(f\"- Token 1 attends to itself (1.0) ‚Üí gets its own value\")\n",
    "print(f\"- Token 3 attends to tokens 0&1 ‚Üí gets mixed information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Position Encoding\n",
    "print(\"\\n2. Position Encoding Effects\")\n",
    "\n",
    "d_model = 16\n",
    "pos_encoder = SinusoidalPositionEncoding(d_model, max_seq_len=20)\n",
    "\n",
    "# Get position encodings for first 8 positions\n",
    "positions = pos_encoder(8)\n",
    "\n",
    "# Visualize position encoding patterns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot position encodings as heatmap\n",
    "sns.heatmap(positions.T, cmap='RdBu_r', center=0, ax=ax1, cbar=True)\n",
    "ax1.set_title('Position Encodings\\n(dimensions √ó positions)')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Dimension')\n",
    "\n",
    "# Plot specific dimensions over positions\n",
    "for dim in [0, 2, 4, 6]:\n",
    "    ax2.plot(positions[:, dim].numpy(), label=f'Dim {dim}')\n",
    "ax2.set_title('Position Encoding Patterns')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Position encoding shape: {positions.shape}\")\n",
    "print(f\"Each position gets a unique encoding vector\")\n",
    "print(f\"Different dimensions have different frequencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: GELU Activation Analysis\n",
    "print(\"\\n3. GELU Activation Function\")\n",
    "\n",
    "gelu = GELU(approximation=\"tanh\")\n",
    "analysis = gelu.analyze_activation_properties(x_range=(-3, 3), n_points=1000)\n",
    "\n",
    "# Plot GELU vs ReLU\n",
    "x = torch.tensor(analysis['activation_values']['x'])\n",
    "y_gelu = torch.tensor(analysis['activation_values']['y'])\n",
    "y_relu = torch.tensor(analysis['comparison_with_relu']['relu_output'])\n",
    "y_derivative = torch.tensor(analysis['activation_values']['derivative'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Activation functions\n",
    "ax1.plot(x, y_gelu, label='GELU', linewidth=2)\n",
    "ax1.plot(x, y_relu, label='ReLU', linewidth=2, alpha=0.7)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "ax1.axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "ax1.set_title('GELU vs ReLU Activation')\n",
    "ax1.set_xlabel('Input')\n",
    "ax1.set_ylabel('Output')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# GELU derivative\n",
    "ax2.plot(x, y_derivative, label='GELU derivative', color='orange', linewidth=2)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "ax2.set_title('GELU Derivative (Gradient)')\n",
    "ax2.set_xlabel('Input')\n",
    "ax2.set_ylabel('Derivative')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Key GELU properties:\")\n",
    "for prop, value in analysis['properties'].items():\n",
    "    print(f\"- {prop}: {value}\")\n",
    "print(f\"\\nGELU is smoother than ReLU and allows small negative values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Part 2: Building the Complete Transformer\n",
    "\n",
    "Now let's build and analyze a complete transformer model step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Complete Transformer Construction\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create educational configuration\n",
    "config = TransformerConfig.educational_config()\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- d_model: {config.d_model}\")\n",
    "print(f\"- n_layers: {config.n_layers}\")\n",
    "print(f\"- n_heads: {config.n_heads}\")\n",
    "print(f\"- d_mlp: {config.d_mlp}\")\n",
    "print(f\"- vocab_size: {config.vocab_size}\")\n",
    "print(f\"- max_position_embeddings: {config.max_position_embeddings}\")\n",
    "\n",
    "# Create the model\n",
    "model = EducationalTransformer(config)\n",
    "model.eval()\n",
    "\n",
    "# Get model info\n",
    "model_info = model.get_model_info()\n",
    "capacity = model_info['model_capacity']\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"- Total parameters: {capacity['total_parameters']:,}\")\n",
    "print(f\"- Model size: {capacity['model_size_mb']:.2f} MB\")\n",
    "print(f\"- Parameters per layer: {capacity['parameters_per_layer']:,}\")\n",
    "\n",
    "print(f\"\\nParameter Distribution:\")\n",
    "total_params = capacity['total_parameters']\n",
    "for component, params in capacity['component_distribution'].items():\n",
    "    percentage = (params / total_params) * 100\n",
    "    print(f\"- {component}: {params:,} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create educational input sequence\n",
    "print(\"\\nüìù Creating Educational Input Sequence\")\n",
    "\n",
    "# Create a simple, interpretable sequence\n",
    "seq_len = 12\n",
    "input_ids = torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]])  # Sequential tokens\n",
    "token_labels = [f\"tok_{i}\" for i in range(seq_len)]\n",
    "\n",
    "print(f\"Input sequence: {input_ids.tolist()[0]}\")\n",
    "print(f\"Token labels: {token_labels}\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "# Forward pass with comprehensive analysis\n",
    "print(\"\\nüîÑ Forward Pass with Analysis\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Basic forward pass\n",
    "    logits = model(input_ids)\n",
    "    \n",
    "    # Forward pass with caching\n",
    "    logits_cached, cache = model.run_with_cache(input_ids)\n",
    "    \n",
    "    # Comprehensive analysis\n",
    "    analysis = model.analyze_model_behavior(input_ids, token_labels=token_labels)\n",
    "\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Cached activations: {len(cache)}\")\n",
    "print(f\"Results match: {torch.allclose(logits, logits_cached)}\")\n",
    "\n",
    "# Show top predictions for last position\n",
    "last_logits = logits[0, -1]  # Last position logits\n",
    "probs = F.softmax(last_logits, dim=-1)\n",
    "top_5 = torch.topk(probs, 5)\n",
    "\n",
    "print(f\"\\nTop 5 predictions for next token:\")\n",
    "for i, (prob, token_id) in enumerate(zip(top_5.values, top_5.indices)):\n",
    "    print(f\"{i+1}. Token {token_id.item()}: {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåä Part 3: Residual Stream Analysis\n",
    "\n",
    "The residual stream is the \"highway\" through which information flows in transformers. Let's analyze how information evolves layer by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåä Residual Stream Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Extract global analysis\n",
    "global_analysis = analysis['global_analysis']\n",
    "\n",
    "# 1. Layer evolution analysis\n",
    "if 'layer_evolution' in global_analysis:\n",
    "    evolution = global_analysis['layer_evolution']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot layer changes\n",
    "    layer_indices = range(1, len(evolution['layer_changes']) + 1)\n",
    "    ax1.bar(layer_indices, evolution['layer_changes'], alpha=0.7)\n",
    "    ax1.set_title('Representation Changes by Layer')\n",
    "    ax1.set_xlabel('Layer Transition')\n",
    "    ax1.set_ylabel('Change Magnitude')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot layer similarities\n",
    "    ax2.plot(layer_indices, evolution['layer_similarities'], 'o-', linewidth=2)\n",
    "    ax2.set_title('Layer-to-Layer Similarity')\n",
    "    ax2.set_xlabel('Layer Transition')\n",
    "    ax2.set_ylabel('Cosine Similarity')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Total change through model: {evolution['total_change']:.4f}\")\n",
    "    print(f\"Layer changes: {[f'{c:.3f}' for c in evolution['layer_changes']]}\")\n",
    "    print(f\"Layer similarities: {[f'{s:.3f}' for s in evolution['layer_similarities']]}\")\n",
    "\n",
    "# 2. Component contribution analysis\n",
    "print(\"\\nüîß Component Contributions by Layer\")\n",
    "\n",
    "attention_contributions = []\n",
    "mlp_contributions = []\n",
    "layer_names = []\n",
    "\n",
    "for layer_key, layer_info in analysis['layer_analysis'].items():\n",
    "    if 'residual_stream_analysis' in layer_info:\n",
    "        residual = layer_info['residual_stream_analysis']\n",
    "        component_mag = residual['component_magnitudes']\n",
    "        \n",
    "        attention_contributions.append(component_mag['relative_attention_contribution'])\n",
    "        mlp_contributions.append(component_mag['relative_mlp_contribution'])\n",
    "        layer_names.append(layer_key.replace('layer_', 'L'))\n",
    "\n",
    "if attention_contributions:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(layer_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, attention_contributions, width, label='Attention', alpha=0.8)\n",
    "    ax.bar(x + width/2, mlp_contributions, width, label='MLP', alpha=0.8)\n",
    "    \n",
    "    ax.set_title('Component Contributions by Layer')\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Relative Contribution')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(layer_names)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Attention contributions: {[f'{c:.3f}' for c in attention_contributions]}\")\n",
    "    print(f\"MLP contributions: {[f'{c:.3f}' for c in mlp_contributions]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÅÔ∏è Part 4: Attention Pattern Analysis\n",
    "\n",
    "Let's dive deep into attention patterns to understand what the model is learning to attend to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üëÅÔ∏è Attention Pattern Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get attention evolution analysis\n",
    "if 'attention_evolution' in global_analysis:\n",
    "    attn_evolution = global_analysis['attention_evolution']\n",
    "    \n",
    "    # Plot attention entropy evolution\n",
    "    if 'attention_entropy_evolution' in attn_evolution:\n",
    "        entropy_data = attn_evolution['attention_entropy_evolution']\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Entropy by layer\n",
    "        ax1.plot(range(len(entropy_data['entropies'])), entropy_data['entropies'], 'o-', linewidth=2)\n",
    "        ax1.set_title('Attention Entropy by Layer')\n",
    "        ax1.set_xlabel('Layer')\n",
    "        ax1.set_ylabel('Entropy (bits)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Entropy changes\n",
    "        if len(entropy_data['entropy_changes']) > 0:\n",
    "            ax2.bar(range(1, len(entropy_data['entropy_changes']) + 1), entropy_data['entropy_changes'], alpha=0.7)\n",
    "            ax2.set_title('Entropy Changes Between Layers')\n",
    "            ax2.set_xlabel('Layer Transition')\n",
    "            ax2.set_ylabel('Entropy Change')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Attention entropies: {[f'{e:.2f}' for e in entropy_data['entropies']]}\")\n",
    "        print(f\"Higher entropy = more distributed attention\")\n",
    "        print(f\"Lower entropy = more focused attention\")\n",
    "\n",
    "# Visualize attention patterns for each layer\n",
    "print(\"\\nüîç Layer-by-Layer Attention Patterns\")\n",
    "\n",
    "# Get attention weights from cache\n",
    "attention_layers = []\n",
    "for i in range(config.n_layers):\n",
    "    attn_key = f'layer_{i}_attn_weights'\n",
    "    if attn_key in cache:\n",
    "        attention_layers.append(cache[attn_key])\n",
    "\n",
    "if attention_layers:\n",
    "    # Plot attention patterns for each layer\n",
    "    n_layers = len(attention_layers)\n",
    "    n_heads = attention_layers[0].shape[1]\n",
    "    \n",
    "    # Show first head of each layer\n",
    "    fig, axes = plt.subplots(1, min(n_layers, 4), figsize=(4*min(n_layers, 4), 4))\n",
    "    if n_layers == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for layer_idx in range(min(n_layers, 4)):\n",
    "        # Average attention across batch and take first head\n",
    "        attn_pattern = attention_layers[layer_idx][0, 0].numpy()  # [seq_len, seq_len]\n",
    "        \n",
    "        im = axes[layer_idx].imshow(attn_pattern, cmap='Blues', aspect='auto')\n",
    "        axes[layer_idx].set_title(f'Layer {layer_idx}\\nHead 0 Attention')\n",
    "        axes[layer_idx].set_xlabel('Key Position')\n",
    "        axes[layer_idx].set_ylabel('Query Position')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=axes[layer_idx], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze head specialization\n",
    "if 'head_specialization_evolution' in attn_evolution:\n",
    "    head_spec = attn_evolution['head_specialization_evolution']\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(len(head_spec['head_diversities'])), head_spec['head_diversities'], 'o-', linewidth=2)\n",
    "    plt.title('Head Specialization Across Layers')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Average Head Correlation')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Head correlations by layer: {[f'{d:.3f}' for d in head_spec['head_diversities']]}\")\n",
    "    print(f\"Lower correlation = more specialized heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Part 5: Mechanistic Interpretability Tools\n",
    "\n",
    "Now let's explore the mechanistic interpretability tools that allow us to understand what the model is doing internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Mechanistic Interpretability Tools\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create hook manager for interventions\n",
    "hook_manager = HookManager(model)\n",
    "\n",
    "# Demo 1: Simple activation intervention\n",
    "print(\"\\n1. Activation Intervention Demo\")\n",
    "\n",
    "# Create test input\n",
    "test_input = torch.tensor([[0, 1, 2, 3, 4]])\n",
    "\n",
    "# Baseline forward pass\n",
    "with torch.no_grad():\n",
    "    baseline_logits = model(test_input)\n",
    "    baseline_probs = F.softmax(baseline_logits[0, -1], dim=-1)\n",
    "    baseline_top = torch.argmax(baseline_probs).item()\n",
    "\n",
    "print(f\"Baseline prediction: token {baseline_top} (prob: {baseline_probs[baseline_top]:.4f})\")\n",
    "\n",
    "# Add intervention: scale down attention output\n",
    "def scale_attention(activation):\n",
    "    return activation * 0.3  # Significantly reduce attention contribution\n",
    "\n",
    "hook_manager.add_intervention_hook(\n",
    "    'blocks.0.attn.hook_z',\n",
    "    scale_attention,\n",
    "    \"Scale attention output by 0.3\"\n",
    ")\n",
    "\n",
    "# Forward pass with intervention\n",
    "with torch.no_grad():\n",
    "    intervention_logits = model(test_input)\n",
    "    intervention_probs = F.softmax(intervention_logits[0, -1], dim=-1)\n",
    "    intervention_top = torch.argmax(intervention_probs).item()\n",
    "\n",
    "print(f\"With attention scaling: token {intervention_top} (prob: {intervention_probs[intervention_top]:.4f})\")\n",
    "print(f\"Prediction changed: {baseline_top != intervention_top}\")\n",
    "\n",
    "# Show intervention summary\n",
    "intervention_summary = hook_manager.get_intervention_summary()\n",
    "print(f\"\\nIntervention summary:\")\n",
    "print(f\"- Total interventions: {intervention_summary['total_interventions']}\")\n",
    "print(f\"- Average change norm: {intervention_summary['average_change_norm']:.4f}\")\n",
    "\n",
    "hook_manager.clear_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Activation Patching Experiment\n",
    "print(\"\\n2. Activation Patching Experiment\")\n",
    "\n",
    "patcher = ActivationPatcher(model)\n",
    "\n",
    "# Create clean and corrupted inputs\n",
    "clean_input = torch.tensor([[1, 2, 3, 4, 5]])     # Sequential pattern\n",
    "corrupted_input = torch.tensor([[1, 2, 3, 4, 99]]) # Last token corrupted\n",
    "\n",
    "print(f\"Clean input: {clean_input.tolist()[0]}\")\n",
    "print(f\"Corrupted input: {corrupted_input.tolist()[0]}\")\n",
    "\n",
    "# Define metric: logit for the expected next token (6)\n",
    "def next_token_metric(logits):\n",
    "    return logits[0, -1, 6].item()  # Logit for token 6\n",
    "\n",
    "# Test different hook points\n",
    "hook_points = [\n",
    "    'blocks.0.hook_resid_post',\n",
    "    'blocks.1.hook_resid_post',\n",
    "    'blocks.0.attn.hook_z',\n",
    "    'blocks.0.mlp.hook_post'\n",
    "]\n",
    "\n",
    "patch_results = []\n",
    "\n",
    "for hook_point in hook_points:\n",
    "    try:\n",
    "        result = patcher.patch_activation(\n",
    "            clean_input, corrupted_input, hook_point, next_token_metric\n",
    "        )\n",
    "        patch_results.append((hook_point, result))\n",
    "        \n",
    "        print(f\"\\n{hook_point}:\")\n",
    "        print(f\"  Clean metric: {result['clean_metric']:.4f}\")\n",
    "        print(f\"  Corrupted metric: {result['corrupted_metric']:.4f}\")\n",
    "        print(f\"  Patched metric: {result['patched_metric']:.4f}\")\n",
    "        print(f\"  Recovery ratio: {result['recovery_ratio']:.4f}\")\n",
    "        print(f\"  Interpretation: {result['analysis']['interpretation']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{hook_point}: Error - {e}\")\n",
    "\n",
    "# Visualize patching results\n",
    "if patch_results:\n",
    "    hook_names = [name.split('.')[-1] for name, _ in patch_results]\n",
    "    recovery_ratios = [result['recovery_ratio'] for _, result in patch_results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(range(len(hook_names)), recovery_ratios, alpha=0.7)\n",
    "    plt.title('Activation Patching Results\\n(Recovery Ratio by Component)')\n",
    "    plt.xlabel('Component')\n",
    "    plt.ylabel('Recovery Ratio')\n",
    "    plt.xticks(range(len(hook_names)), hook_names, rotation=45)\n",
    "    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='50% recovery')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color bars based on importance\n",
    "    for bar, ratio in zip(bars, recovery_ratios):\n",
    "        if ratio > 0.5:\n",
    "            bar.set_color('green')\n",
    "        elif ratio > 0.2:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: Ablation Study\n",
    "print(\"\\n3. Component Ablation Study\")\n",
    "\n",
    "ablator = AblationAnalyzer(model)\n",
    "\n",
    "# Test input for ablation\n",
    "test_input = torch.tensor([[0, 1, 2, 3, 4, 5]])\n",
    "\n",
    "# Define metric: output norm (simple measure of model activity)\n",
    "def output_norm_metric(logits):\n",
    "    return torch.norm(logits).item()\n",
    "\n",
    "# Components to ablate\n",
    "components_to_ablate = [\n",
    "    ('blocks.0.attn.hook_z', 'Layer 0 Attention'),\n",
    "    ('blocks.0.mlp.hook_post', 'Layer 0 MLP'),\n",
    "    ('blocks.1.attn.hook_z', 'Layer 1 Attention'),\n",
    "    ('blocks.1.mlp.hook_post', 'Layer 1 MLP'),\n",
    "]\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "for hook_name, component_name in components_to_ablate:\n",
    "    try:\n",
    "        result = ablator.zero_ablation(test_input, hook_name, output_norm_metric)\n",
    "        ablation_results.append((component_name, result))\n",
    "        \n",
    "        print(f\"\\n{component_name}:\")\n",
    "        print(f\"  Baseline metric: {result['baseline_metric']:.4f}\")\n",
    "        print(f\"  Ablated metric: {result['ablated_metric']:.4f}\")\n",
    "        print(f\"  Relative effect: {result['relative_effect']:.4f}\")\n",
    "        print(f\"  Importance score: {result['importance_score']:.4f}\")\n",
    "        print(f\"  Interpretation: {result['interpretation']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{component_name}: Error - {e}\")\n",
    "\n",
    "# Visualize ablation results\n",
    "if ablation_results:\n",
    "    component_names = [name for name, _ in ablation_results]\n",
    "    importance_scores = [result['importance_score'] for _, result in ablation_results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(range(len(component_names)), importance_scores, alpha=0.7)\n",
    "    plt.title('Component Importance (Ablation Study)')\n",
    "    plt.xlabel('Component')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.xticks(range(len(component_names)), component_names, rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color bars based on importance\n",
    "    for bar, score in zip(bars, importance_scores):\n",
    "        if score > 0.5:\n",
    "            bar.set_color('red')      # Critical\n",
    "        elif score > 0.2:\n",
    "            bar.set_color('orange')   # Important\n",
    "        elif score > 0.05:\n",
    "            bar.set_color('yellow')   # Minor\n",
    "        else:\n",
    "            bar.set_color('green')    # Negligible\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sort by importance\n",
    "    sorted_results = sorted(ablation_results, key=lambda x: x[1]['importance_score'], reverse=True)\n",
    "    print(f\"\\nüìä Component Ranking (Most to Least Important):\")\n",
    "    for i, (name, result) in enumerate(sorted_results, 1):\n",
    "        print(f\"{i}. {name}: {result['importance_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 6: Educational Insights and Takeaways\n",
    "\n",
    "Let's summarize the key insights we've gained from building and analyzing our transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Educational Insights and Key Takeaways\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a summary visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Model capacity breakdown\n",
    "capacity = model_info['model_capacity']\n",
    "components = list(capacity['component_distribution'].keys())\n",
    "params = list(capacity['component_distribution'].values())\n",
    "\n",
    "ax1.pie(params, labels=components, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Parameter Distribution')\n",
    "\n",
    "# 2. Layer evolution (if available)\n",
    "if 'layer_evolution' in global_analysis:\n",
    "    evolution = global_analysis['layer_evolution']\n",
    "    ax2.plot(range(1, len(evolution['layer_changes']) + 1), evolution['layer_changes'], 'o-', linewidth=2)\n",
    "    ax2.set_title('Representation Changes by Layer')\n",
    "    ax2.set_xlabel('Layer Transition')\n",
    "    ax2.set_ylabel('Change Magnitude')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Attention entropy evolution (if available)\n",
    "if 'attention_evolution' in global_analysis and 'attention_entropy_evolution' in global_analysis['attention_evolution']:\n",
    "    entropy_data = global_analysis['attention_evolution']['attention_entropy_evolution']\n",
    "    ax3.plot(range(len(entropy_data['entropies'])), entropy_data['entropies'], 's-', linewidth=2, color='purple')\n",
    "    ax3.set_title('Attention Entropy by Layer')\n",
    "    ax3.set_xlabel('Layer')\n",
    "    ax3.set_ylabel('Entropy (bits)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Component contributions (if available)\n",
    "if attention_contributions and mlp_contributions:\n",
    "    x = np.arange(len(layer_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax4.bar(x - width/2, attention_contributions, width, label='Attention', alpha=0.8)\n",
    "    ax4.bar(x + width/2, mlp_contributions, width, label='MLP', alpha=0.8)\n",
    "    ax4.set_title('Component Contributions')\n",
    "    ax4.set_xlabel('Layer')\n",
    "    ax4.set_ylabel('Relative Contribution')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(layer_names)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\nüéì Key Educational Insights:\")\n",
    "print(\"\\n1. üèóÔ∏è Architecture Understanding:\")\n",
    "print(f\"   ‚Ä¢ Transformers use {config.n_heads} attention heads to process information in parallel\")\n",
    "print(f\"   ‚Ä¢ Each head has dimension {config.d_model // config.n_heads}, allowing specialized attention patterns\")\n",
    "print(f\"   ‚Ä¢ MLP blocks expand representations to {config.d_mlp}D before contracting back\")\n",
    "print(f\"   ‚Ä¢ Position encoding is crucial - without it, transformers can't distinguish token order\")\n",
    "\n",
    "print(\"\\n2. üåä Residual Stream Insights:\")\n",
    "if 'layer_evolution' in global_analysis:\n",
    "    evolution = global_analysis['layer_evolution']\n",
    "    total_change = evolution['total_change']\n",
    "    print(f\"   ‚Ä¢ Total representation change through model: {total_change:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Information flows through residual stream while components make targeted edits\")\n",
    "    print(f\"   ‚Ä¢ Each layer modifies representations while preserving important information\")\n",
    "\n",
    "print(\"\\n3. üëÅÔ∏è Attention Pattern Insights:\")\n",
    "if 'attention_evolution' in global_analysis:\n",
    "    attn_evolution = global_analysis['attention_evolution']\n",
    "    if 'attention_entropy_evolution' in attn_evolution:\n",
    "        entropies = attn_evolution['attention_entropy_evolution']['entropies']\n",
    "        print(f\"   ‚Ä¢ Attention entropy varies by layer: {[f'{e:.2f}' for e in entropies]}\")\n",
    "        print(f\"   ‚Ä¢ Higher entropy = more distributed attention, lower = more focused\")\n",
    "        print(f\"   ‚Ä¢ Different layers learn different attention strategies\")\n",
    "\n",
    "print(\"\\n4. üî¨ Interpretability Insights:\")\n",
    "print(f\"   ‚Ä¢ Hook system allows precise intervention at any point in computation\")\n",
    "print(f\"   ‚Ä¢ Activation patching reveals causal importance of components\")\n",
    "print(f\"   ‚Ä¢ Ablation studies quantify how much each component contributes\")\n",
    "print(f\"   ‚Ä¢ Components can be ranked by importance for specific behaviors\")\n",
    "\n",
    "print(\"\\n5. üßÆ Mathematical Foundations:\")\n",
    "print(f\"   ‚Ä¢ Scaled dot-product attention prevents saturation with ‚àöd_k scaling\")\n",
    "print(f\"   ‚Ä¢ GELU provides smooth, differentiable non-linearity\")\n",
    "print(f\"   ‚Ä¢ Layer normalization stabilizes training and activations\")\n",
    "print(f\"   ‚Ä¢ Causal masking ensures autoregressive property\")\n",
    "\n",
    "print(\"\\nüéØ Research Applications:\")\n",
    "print(f\"   ‚Ä¢ This implementation is fully compatible with TransformerLens\")\n",
    "print(f\"   ‚Ä¢ All components include educational analysis tools\")\n",
    "print(f\"   ‚Ä¢ Hook system enables sophisticated interpretability research\")\n",
    "print(f\"   ‚Ä¢ Educational features help build intuition about transformer behavior\")\n",
    "\n",
    "print(\"\\n‚úÖ ARENA Chapter 1.1 Learning Objectives Achieved:\")\n",
    "print(f\"   ‚úì Built transformer from mathematical foundations\")\n",
    "print(f\"   ‚úì Understood attention mechanisms and multi-head architecture\")\n",
    "print(f\"   ‚úì Implemented and analyzed residual stream information flow\")\n",
    "print(f\"   ‚úì Created mechanistic interpretability tools\")\n",
    "print(f\"   ‚úì Gained practical experience with transformer analysis\")\n",
    "print(f\"   ‚úì Connected implementation to research-grade interpretability methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps and Advanced Exploration\n",
    "\n",
    "Congratulations! You've successfully built and analyzed a complete transformer implementation following the ARENA Chapter 1.1 curriculum. Here are some advanced directions to explore:\n",
    "\n",
    "### üî¨ Advanced Interpretability\n",
    "- **Circuit Analysis**: Use the tools to identify specific computational circuits\n",
    "- **Induction Head Analysis**: Look for heads that implement copying behaviors\n",
    "- **Feature Visualization**: Analyze what different neurons and attention heads specialize in\n",
    "- **Causal Mediation**: Use activation patching to understand causal relationships\n",
    "\n",
    "### üß™ Experimental Extensions\n",
    "- **Different Architectures**: Try different layer norms, activations, or attention patterns\n",
    "- **Training Dynamics**: Train the model and analyze how interpretability changes\n",
    "- **Scaling Laws**: Experiment with different model sizes and analyze the effects\n",
    "- **Task-Specific Analysis**: Train on specific tasks and analyze learned behaviors\n",
    "\n",
    "### üîó Integration with Research Tools\n",
    "- **TransformerLens Integration**: Use with the full TransformerLens library\n",
    "- **Benchmark Comparisons**: Compare with other transformer implementations\n",
    "- **Research Reproduction**: Use tools to reproduce interpretability papers\n",
    "- **Novel Research**: Apply techniques to discover new insights\n",
    "\n",
    "### üìö Educational Applications\n",
    "- **Interactive Demos**: Create interactive visualizations for teaching\n",
    "- **Course Materials**: Adapt for transformer courses and workshops\n",
    "- **Research Training**: Use as foundation for interpretability research\n",
    "- **Open Source Contributions**: Contribute improvements back to the community\n",
    "\n",
    "**üéâ You now have a deep, practical understanding of transformer architectures and the tools to explore their inner workings!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}